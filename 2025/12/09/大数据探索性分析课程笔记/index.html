
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="utf-8" />
    <title>大数据探索性分析-数据配平&amp;特征变换&amp;特征选择 | yty的博客</title>
    <meta name="author" content="Leo yu yty" />
    <meta name="description" content="Hello Everyone This is my website" />
    <meta name="keywords" content="" />
    <meta
        name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0"
    />
    <link rel="icon" href="/images/xnylh.webp" />
    <link rel="preconnect" href="https://s4.zstatic.net" />
<script src="https://s4.zstatic.net/ajax/libs/vue/3.3.7/vue.global.prod.min.js"></script>
<link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/font-awesome/6.4.2/css/all.min.css" />
<link rel="preconnect" href="https://fonts.googleapis.cn" />
<link rel="preconnect" href="https://fonts.gstatic.cn" crossorigin />
<link
    rel="stylesheet"
    href="https://fonts.googleapis.cn/css2?family=Fira+Code:wght@400;500;600;700&family=Lexend:wght@400;500;600;700;800;900&family=Noto+Sans+SC:wght@400;500;600;700;800;900&display=swap"
/>
<script> const mixins = {}; </script>

<script src="https://polyfill.alicdn.com/v3/polyfill.min.js?features=default"></script>


<script src="https://s4.zstatic.net/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script src="https://s4.zstatic.net/ajax/libs/highlightjs-line-numbers.js/2.8.0/highlightjs-line-numbers.min.js"></script>
<link
    rel="stylesheet"
    href="https://s4.zstatic.net/ajax/libs/highlight.js/11.9.0/styles/github.min.css"
/>
<script src="/js/lib/highlight.js"></script>



<script src="/js/lib/preview.js"></script>









<link rel="stylesheet" href="/css/main.css" />

<meta name="generator" content="Hexo 8.1.1"></head>
<body>
    <div id="layout">
        <transition name="fade">
            <div id="loading" v-show="loading">
                <div id="loading-circle">
                    <h2>LOADING</h2>
                    <p>加载过慢请开启缓存 浏览器默认开启</p>
                    <img src="/images/loading.gif" />
                </div>
            </div>
        </transition>
        <div id="menu" :class="{ hidden: hiddenMenu, 'menu-color': menuColor}">
    <nav id="desktop-menu">
        <a class="title" href="/">
            <span>YTY的博客</span>
        </a>
        
        <a href="/">
            <i class="fa-solid fa-house fa-fw"></i>
            <span>&ensp;主页</span>
        </a>
        
        <a href="/about/">
            <i class="fa-solid fa-id-card fa-fw"></i>
            <span>&ensp;关于</span>
        </a>
        
        <a href="/archives/">
            <i class="fa-solid fa-box-archive fa-fw"></i>
            <span>&ensp;文章</span>
        </a>
        
        <a href="/categories/">
            <i class="fa-solid fa-bookmark fa-fw"></i>
            <span>&ensp;分类</span>
        </a>
        
        <a href="/tags/">
            <i class="fa-solid fa-tags fa-fw"></i>
            <span>&ensp;标签</span>
        </a>
        
    </nav>
    <nav id="mobile-menu">
        <div class="title" @click="showMenuItems = !showMenuItems">
            <i class="fa-solid fa-bars fa-fw"></i>
            <span>&emsp;YTY的博客</span>
        </div>
        <transition name="slide">
            <div class="items" v-show="showMenuItems">
                
                <a href="/">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-house fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">主页</div>
                    </div>
                </a>
                
                <a href="/about/">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-id-card fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">关于</div>
                    </div>
                </a>
                
                <a href="/archives/">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-box-archive fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">文章</div>
                    </div>
                </a>
                
                <a href="/categories/">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-bookmark fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">分类</div>
                    </div>
                </a>
                
                <a href="/tags/">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-tags fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">标签</div>
                    </div>
                </a>
                
            </div>
        </transition>
    </nav>
</div>
<transition name="fade">
    <div id="menu-curtain" @click="showMenuItems = !showMenuItems" v-show="showMenuItems"></div>
</transition>

        <div id="main" :class="loading ? 'into-enter-from': 'into-enter-active'">
            <div class="article">
    <div>
        <h1>大数据探索性分析-数据配平&amp;特征变换&amp;特征选择</h1>
    </div>
    <div class="info">
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2025/12/9
        </span>
        
        <span class="category">
            <a href="/categories/%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                复习笔记
            </a>
        </span>
        
        
        <span class="tags">
            <span class="icon">
                <i class="fa-solid fa-tags fa-fw"></i>
            </span>
            
            
            <span class="tag">
                
                <a href="/tags/%E7%AC%94%E8%AE%B0/" style="color: #03a9f4">
                    笔记
                </a>
            </span>
            
            <span class="tag">
                
                <a href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90/" style="color: #ff7d73">
                    大数据探索性分析
                </a>
            </span>
            
        </span>
        
    </div>
    
    <div class="content" v-pre>
        <h2 id="摘要部分"><a href="#摘要部分" class="headerlink" title="摘要部分"></a>摘要部分</h2><h3 id="一、数据配平"><a href="#一、数据配平" class="headerlink" title="一、数据配平"></a>一、数据配平</h3><h4 id="处理类别不平衡问题"><a href="#处理类别不平衡问题" class="headerlink" title="处理类别不平衡问题"></a>处理类别不平衡问题</h4><h4 id="1、欠采样-过采样"><a href="#1、欠采样-过采样" class="headerlink" title="1、欠采样 &amp; 过采样"></a>1、欠采样 &amp; 过采样</h4><ul>
<li>欠采样：删多数类样本</li>
<li>过采样：补充少数类样本</li>
</ul>
<h4 id="2、加权损失函数"><a href="#2、加权损失函数" class="headerlink" title="2、加权损失函数"></a>2、加权损失函数</h4><p>给少数类样本分配更高的权重</p>
<h3 id="二、模型评估与优化"><a href="#二、模型评估与优化" class="headerlink" title="二、模型评估与优化"></a>二、模型评估与优化</h3><h4 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h4><img src="/2025/12/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/4.png" class="" title="图片描述">

<ul>
<li><p>准确率</p>
</li>
<li><p>精准率</p>
</li>
<li><p>召回率</p>
</li>
<li><p>ROC曲线 &amp; AUC</p>
</li>
</ul>
<h3 id="三、特征变化"><a href="#三、特征变化" class="headerlink" title="三、特征变化"></a>三、特征变化</h3><h4 id="1、特征工程"><a href="#1、特征工程" class="headerlink" title="1、特征工程"></a>1、特征工程</h4><ul>
<li>特征变换：改变已有特征</li>
<li>特征选择：筛选出最重要的特征</li>
<li>特征提取：把多个特征浓缩为少数几个</li>
</ul>
<h4 id="2、解决尺度差异的两种办法"><a href="#2、解决尺度差异的两种办法" class="headerlink" title="2、解决尺度差异的两种办法"></a>2、解决尺度差异的两种办法</h4><ul>
<li>标准化：消除量纲的影响</li>
<li>归一化（min-max归一化）</li>
</ul>
<h4 id="3、改善数据分布的三种办法"><a href="#3、改善数据分布的三种办法" class="headerlink" title="3、改善数据分布的三种办法"></a>3、改善数据分布的三种办法</h4><ul>
<li><p>对数变化（取对数 &#x2F; 取平方根）</p>
</li>
<li><p>BOX-COX变化（非正态分布变为近正态分布）</p>
</li>
<li><p>离散化（分箱）</p>
<ul>
<li><p>等宽分箱</p>
</li>
<li><p>等频分箱</p>
</li>
<li><p>离散化的核心作用：引入非正态分布</p>
</li>
</ul>
</li>
</ul>
<h4 id="4、类别型数据的特征编码"><a href="#4、类别型数据的特征编码" class="headerlink" title="4、类别型数据的特征编码"></a>4、类别型数据的特征编码</h4><ul>
<li>独热编码</li>
<li>序数编码</li>
<li>频率编码</li>
<li>目标编码</li>
</ul>
<h3 id="四、特征选择"><a href="#四、特征选择" class="headerlink" title="四、特征选择"></a>四、特征选择</h3><p>避免维数灾难：特征过多</p>
<h4 id="1、过滤法"><a href="#1、过滤法" class="headerlink" title="1、过滤法"></a>1、过滤法</h4><ul>
<li>相关系数法（皮尔逊相关系数）：衡量数值型变量的线性相关程度</li>
<li>卡方检验法</li>
<li>单变量方差分析</li>
</ul>
<h4 id="2、封装法"><a href="#2、封装法" class="headerlink" title="2、封装法"></a>2、封装法</h4><ul>
<li>递归特征消除</li>
<li>前向选择</li>
<li>后向消除</li>
</ul>
<h4 id="3、嵌入法"><a href="#3、嵌入法" class="headerlink" title="3、嵌入法"></a>3、嵌入法</h4><ul>
<li>正则化：正则化会在模型的【损失函数】中添加一个额外的正则项，这个正则项会“惩罚”模型使用过多或过大的系数</li>
<li>Lasso回归（L1正则化）：将正则化的惩罚项换为：<strong>所有特征系数的绝对值之和</strong></li>
</ul>
<span id="more"></span>









<h2 id="数据配平"><a href="#数据配平" class="headerlink" title="数据配平"></a>数据配平</h2><h3 id="一、数据配平定义"><a href="#一、数据配平定义" class="headerlink" title="一、数据配平定义"></a>一、数据配平定义</h3><p>数据配平是针对于机器学习中“类别不平衡”问题的关键预处理技术，通过调整不同类别的样本分布比例，使模型能够提升对少数类样本的识别能力。</p>
<p>例如：想象你要训练一个 “识别人” 的模型：如果给它 1000 张男生照片、只给 1 张女生照片，模型练完后大概率只会认男生 —— 因为它见太多男生、没学过女生的特征，这就是 “类别不平衡”。</p>
<h5 id="类别不平衡的危害："><a href="#类别不平衡的危害：" class="headerlink" title="类别不平衡的危害："></a>类别不平衡的危害：</h5><ul>
<li>模型对于少数类的预测能力较差</li>
<li>在不平衡数据集上，常用的准确率指标可能误导分析</li>
<li>实际场景中漏检关键少数类</li>
</ul>
<h3 id="二、欠采样-删"><a href="#二、欠采样-删" class="headerlink" title="二、欠采样 - 删"></a>二、欠采样 - 删</h3><h5 id="定义：通过减少多数类样本的样本数量来平衡数据集"><a href="#定义：通过减少多数类样本的样本数量来平衡数据集" class="headerlink" title="定义：通过减少多数类样本的样本数量来平衡数据集"></a>定义：通过减少多数类样本的样本数量来平衡数据集</h5><ul>
<li>随机欠采样：随机删除多数类样本，使两类样本数量接近</li>
<li>基于聚类的欠采样：对多数类样本聚类（K - means），仅保留簇中心样本</li>
</ul>
<p>注：这里K - means聚类就是将多数类分为多个簇，每个簇保留中心样本</p>
<h5 id="欠采样的缺点"><a href="#欠采样的缺点" class="headerlink" title="欠采样的缺点"></a>欠采样的缺点</h5><p>删去样本可能会丢失有价值的信息；同时数据量的减少还会导致模型泛化能力下降</p>
<h5 id="解决欠采样导致的信息缺失方法："><a href="#解决欠采样导致的信息缺失方法：" class="headerlink" title="解决欠采样导致的信息缺失方法："></a>解决欠采样导致的信息缺失方法：</h5><ul>
<li><p>Bagging法：对多数类进行多次有放回的欠采样，从而得到多个不同的训练集，对这些不同训练集建立模型（也叫训练出多个分类器），最后综合得到的结果。</p>
<p>即：多次有放回欠采样，多模型组合</p>
</li>
<li><p>Boosting法：先通过一次欠采样得到训练集，并建立模型训练出一个分类器；然后从一开始的样本集中删掉第一次用掉的这些样本，对剩下的样本再次欠采样重复上面的步骤，最后综合得到结果。</p>
<p>即：逐步删除已经学会的多数类样本，多模型迭代</p>
</li>
</ul>
<h3 id="三、过采样-补"><a href="#三、过采样-补" class="headerlink" title="三、过采样 - 补"></a>三、过采样 - 补</h3><h5 id="定义：增加少数类样本数量来平衡数据集"><a href="#定义：增加少数类样本数量来平衡数据集" class="headerlink" title="定义：增加少数类样本数量来平衡数据集"></a>定义：增加少数类样本数量来平衡数据集</h5><ul>
<li><p>随机过采样：直接复制少数类样本（比如把 100 个少数类复制 9 次变 1000 个）</p>
<ul>
<li>优点：简单快速</li>
<li>缺点：导致样本重复，使模型在新数据上泛化能力差</li>
</ul>
</li>
<li><p>SMOTE方法：仿照少数类样本，造与他们相似的新样本</p>
<ul>
<li>优点：样本多样</li>
<li>缺点：可能偏离真实数据分布</li>
</ul>
</li>
</ul>
<img src="/2025/12/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/1.png" class="" title="图片描述">

<h3 id="四、加权损失函数"><a href="#四、加权损失函数" class="headerlink" title="四、加权损失函数"></a>四、加权损失函数</h3><p>解释：对少数类样本赋予更大的权重</p>
<p>优：不引入新样本，不改变数据分布，模型保持原有结构</p>
<h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>模型预测值和真实值的差</p>
<h2 id="五、对模型的评估和优化"><a href="#五、对模型的评估和优化" class="headerlink" title="五、对模型的评估和优化"></a>五、对模型的评估和优化</h2><p>如何判断分类模型好不好？（特别是当样本不均衡的时候）</p>
<h3 id="1）混淆矩阵"><a href="#1）混淆矩阵" class="headerlink" title="1）混淆矩阵"></a>1）混淆矩阵</h3><img src="/2025/12/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/4.png" class="" title="图片描述">

<p>总结：</p>
<p>情境：二分类问题，所有样本为两类之一：正例、负例</p>
<ul>
<li>TP：真实值为正例；预测值为正例 —— 预测正确</li>
<li>FP：真实值为负例；预测值为正例 —— 将多数样本判为少数样本（误报）</li>
<li>FN：真实值为正例；预测值为负例 —— 漏掉少数样本（漏报）</li>
<li>TN：真实值为负例；预测值为负例 —— 预测正确</li>
</ul>
<h3 id="2）准确率"><a href="#2）准确率" class="headerlink" title="2）准确率"></a>2）准确率</h3><img src="/2025/12/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/5.png" class="" title="图片描述">

<p>即：混淆矩阵（左上+右下）&#x2F;（总样本数）</p>
<h3 id="3）精准率"><a href="#3）精准率" class="headerlink" title="3）精准率"></a>3）精准率</h3><img src="/2025/12/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/6.png" class="" title="图片描述">

<p>即：混淆矩阵（左上）&#x2F;（左上+左下）</p>
<p>注：误报（FP）（左下）成本高的场景，追求高精准率</p>
<h3 id="4）召回率"><a href="#4）召回率" class="headerlink" title="4）召回率"></a>4）召回率</h3><img src="/2025/12/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/7.png" class="" title="图片描述">

<p>即：混淆矩阵（左上）&#x2F;（左上+右上）</p>
<p>注：漏报（FN）（右上）成本高的场景，追求高召回率</p>
<h3 id="5）F1得分-兼顾精准率与召回率"><a href="#5）F1得分-兼顾精准率与召回率" class="headerlink" title="5）F1得分 - 兼顾精准率与召回率"></a>5）F1得分 - 兼顾精准率与召回率</h3><img src="/2025/12/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/8.png" class="" title="图片描述">

<h3 id="6）ROC曲线"><a href="#6）ROC曲线" class="headerlink" title="6）ROC曲线"></a>6）ROC曲线</h3><h5 id="分类阈值"><a href="#分类阈值" class="headerlink" title="分类阈值"></a>分类阈值</h5><p>模型的分类结果依赖于一个分类阈值，如：结果为正例的概率超过0.5就预测为正例。</p>
<h4 id="ROC曲线的两个指标"><a href="#ROC曲线的两个指标" class="headerlink" title="ROC曲线的两个指标"></a>ROC曲线的两个指标</h4><ul>
<li>TPR（True Positive Rate）&#x3D; 召回率 &#x3D; 混淆矩阵（左上）&#x2F;（左上+右上）</li>
<li>FPR（False Positive Rate）（误报率）&#x3D; 混淆矩阵（左下）&#x2F;（左下+右下）</li>
</ul>
<img src="/2025/12/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/10.png" class="" title="图片描述">

<h4 id="ROC曲线画图"><a href="#ROC曲线画图" class="headerlink" title="ROC曲线画图"></a>ROC曲线画图</h4><p>ROC曲线是一条以FPR为横坐标，TPR为纵坐标的曲线。</p>
<p>即：通过不断调整模型的分类阈值，会得到一系列（FPR，TPR）的点，这些点组成了ROC曲线。</p>
<img src="/2025/12/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/9.png" class="" title="图片描述">

<ul>
<li>模型分类阈值高时，靠近左下（0，0）</li>
<li>模型分类阈值低时，靠近右上（1，1）</li>
</ul>
<p>好的二分类模型的ROC曲线应该急剧向上凸，快速接近左上角（即：当FPR低的时候就有高的TPR）</p>
<h4 id="AUC：ROC曲线下面部分的面积"><a href="#AUC：ROC曲线下面部分的面积" class="headerlink" title="AUC：ROC曲线下面部分的面积"></a>AUC：ROC曲线下面部分的面积</h4><p>作用：用于衡量分类器的性能</p>
<ul>
<li>AUC &#x3D; 1：完美分类器</li>
<li>AUC &#x3D; 0.5：随机猜测模型（模型没有任何区分能力）</li>
<li>AUC &lt; 0.5：无效模型</li>
</ul>
<h2 id="特征变换"><a href="#特征变换" class="headerlink" title="特征变换"></a>特征变换</h2><h3 id="一、特征变换定义："><a href="#一、特征变换定义：" class="headerlink" title="一、特征变换定义："></a>一、特征变换定义：</h3><p>将原始数据中的特征转化为更适合模型使用的形式，通过改变特征分布或表现形式，但不改变信息本质。</p>
<h4 id="特征工程："><a href="#特征工程：" class="headerlink" title="特征工程："></a>特征工程：</h4><ul>
<li>特征变换：改变已有特征</li>
<li>特征选择：筛选出最重要的特征</li>
<li>特征提取：把多个特征浓缩为少数几个</li>
</ul>
<h5 id="为什么做特征变换："><a href="#为什么做特征变换：" class="headerlink" title="为什么做特征变换："></a>为什么做特征变换：</h5><ul>
<li>适应模型的尺度敏感性</li>
<li>解决数据分布失衡</li>
<li>建模非线性关系</li>
<li>处理非数值特征</li>
<li>提升建模收敛速度</li>
</ul>
<h3 id="二、解决尺度差异："><a href="#二、解决尺度差异：" class="headerlink" title="二、解决尺度差异："></a>二、解决尺度差异：</h3><h4 id="1）标准化"><a href="#1）标准化" class="headerlink" title="1）标准化"></a>1）标准化</h4><p>即：将原来所有特征全部统一改为 - 均值为0、标准差为1的统一标准</p>
<img src="/2025/12/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/11.png" class="" title="图片描述">

<p>特点：不改变数据分布，只改变尺度和中心；对异常值敏感</p>
<h4 id="2）归一化"><a href="#2）归一化" class="headerlink" title="2）归一化"></a>2）归一化</h4><p>定义：将数据按比例压缩到一个固定范围</p>
<h5 id="Min-Max-Scaling（最小最大归一化）-常用"><a href="#Min-Max-Scaling（最小最大归一化）-常用" class="headerlink" title="Min-Max Scaling（最小最大归一化）- 常用"></a>Min-Max Scaling（最小最大归一化）- 常用</h5><p>将数据按比例压缩到 [0,1] 的范围</p>
<img src="/2025/12/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/12.png" class="" title="图片描述">

<p>缺点：容易受极端值的影响，若数据中存在异常值，归一化后的数据分布可能不均匀</p>
<p>当存在异常值时用鲁棒缩放</p>
<h5 id="最大绝对值缩放"><a href="#最大绝对值缩放" class="headerlink" title="最大绝对值缩放"></a>最大绝对值缩放</h5><p>将数据按比例压缩到 [-1,1]</p>
<img src="/2025/12/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/13.png" class="" title="图片描述">

<h5 id="鲁棒缩放（Robust-Scaling）-针对含异常值的数据"><a href="#鲁棒缩放（Robust-Scaling）-针对含异常值的数据" class="headerlink" title="鲁棒缩放（Robust Scaling）- 针对含异常值的数据"></a>鲁棒缩放（Robust Scaling）- 针对含异常值的数据</h5><img src="/2025/12/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/14.png" class="" title="图片描述">

<p>这里 IQR 指的是四分位距：</p>
<p>简单说就是数据中 “中间 50% 的数的范围”—— 先把数据从小到大排序，取第 75% 位置的数（上四分位数 Q3）减去第 25% 位置的数（下四分位数 Q1），即 IQR &#x3D; Q3 - Q1。</p>
<h3 id="三、改善分布"><a href="#三、改善分布" class="headerlink" title="三、改善分布"></a>三、改善分布</h3><h5 id="1）对数变换-压缩数据尺度"><a href="#1）对数变换-压缩数据尺度" class="headerlink" title="1）对数变换 - 压缩数据尺度"></a>1）对数变换 - 压缩数据尺度</h5><img src="/2025/12/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/15.png" class="" title="图片描述">

<h5 id="2）BOX-COX变换"><a href="#2）BOX-COX变换" class="headerlink" title="2）BOX - COX变换"></a>2）BOX - COX变换</h5><p>目标：将非正态分布数据转换为近似正态分布数据</p>
<img src="/2025/12/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/16.png" class="" title="图片描述">

<h5 id="3）离散化-分箱"><a href="#3）离散化-分箱" class="headerlink" title="3）离散化 &#x3D; 分箱"></a>3）离散化 &#x3D; 分箱</h5><p>定义：将连续不断的数值特征划分为有限个区间；并将每个区间映射为一个离散值的过程</p>
<p>数据预处理阶段：</p>
<ul>
<li><p>**等宽分箱：**将值域范围划分为K个等宽的区间，宽度 &#x3D; （max - min）&#x2F;  K</p>
<ul>
<li>缺点：对异常值敏感；导致数据分布不均</li>
</ul>
</li>
<li><p>**等频分箱：**将数据划分为K个区间，使得每个区间包含样本数相同</p>
<ul>
<li>分界点选择：数据的分位数</li>
</ul>
</li>
<li><p>基于业务知识&#x2F;模型性能的分箱</p>
</li>
</ul>
<h5 id="4）离散化的作用："><a href="#4）离散化的作用：" class="headerlink" title="4）离散化的作用："></a>4）离散化的作用：</h5><ul>
<li>引入非线性关系</li>
<li>改善特征分布 - 将非正态分布的数据转化为(近)均匀分布</li>
<li>提高模型鲁棒性 - 限制离散值与噪声的影响</li>
<li>便于特征交叉与解释 - 特征组合</li>
</ul>
<h3 id="四、类别特征编码"><a href="#四、类别特征编码" class="headerlink" title="四、类别特征编码"></a>四、类别特征编码</h3><h4 id="1）独热编码（One-Hot-Encoding）"><a href="#1）独热编码（One-Hot-Encoding）" class="headerlink" title="1）独热编码（One - Hot Encoding）"></a>1）独热编码（One - Hot Encoding）</h4><p>定义：为每个类别建二值列，样本属于该类别则列值为 1，否则为 0</p>
<p>人话：将类别型特征变为模型能读懂的数字形式，通过给每个类别单独建一列，样本属于哪个类别，对应列标“1”，其他列标“0”</p>
<p>案例：</p>
<img src="/2025/12/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/17.png" class="" title="图片描述">

<p>特点：</p>
<ul>
<li>平等对待所有类别</li>
<li>缺点1：类别数量多时，会增加数据集维度</li>
<li>缺点2：多重共线性（比如 3 个类别列的和永远是 1（因为一个样本只能属于一个类别），会让模型计算出问题）<ul>
<li>解决方法：哑变量编码（去掉其中一列，这一列代表类别用其他列表示）</li>
</ul>
</li>
<li>缺点3：特征重要性分散</li>
</ul>
<h4 id="2）序数编码"><a href="#2）序数编码" class="headerlink" title="2）序数编码"></a>2）序数编码</h4><p>定义：处理”有顺序的类别特征“，把有序类别（如小学→中学→大学）映射成有序整数（1→2→3），保留类别间的顺序关系。</p>
<p>优点：</p>
<ul>
<li>保留顺序信息</li>
<li>避免维度爆炸</li>
</ul>
<p>缺点：</p>
<ul>
<li>隐含”数值距离相等“假设</li>
<li>类别数量多时，过度关注数值大小，忽略类别本身意义</li>
<li>测试集可能出现新类别<ul>
<li>预先定义完整的类别顺序</li>
<li>制定规则处理未知类别</li>
</ul>
</li>
</ul>
<h4 id="3）频率编码"><a href="#3）频率编码" class="headerlink" title="3）频率编码"></a>3）频率编码</h4><p>定义：将分类变量替换为它在数据集中出现的频率</p>
<p>注：频率本身带有预测信息，如：比如 “Chrome 浏览器” 出现频率高，说明它是主流浏览器，可能对应 “年轻用户” 等特征，模型能利用这种 “常见程度” 的信号</p>
<p>适用场景：类别出现频率本身具有信息量时</p>
<p>缺点：丢失类别本身的信息（如果两个不同类别出现频率一样（比如 “Firefox” 和 “Edge” 都出现 200 次，编码都是 0.2），模型会把它们当成同一个特征）</p>
<p>注：只能在训练集上计算频率，防止数据泄露</p>
<h4 id="4）目标编码"><a href="#4）目标编码" class="headerlink" title="4）目标编码"></a>4）目标编码</h4><p>定义：用“类别对应的目标变量统计值”代替类别标签</p>
<p>如：比如预测 “用户是否购买商品”（目标是 “购买 &#x2F; 不购买”），对 “城市” 这个类别特征，计算每个城市的 “购买率”（目标变量的期望值），用这个购买率作为该城市的编码值。</p>
<p>优点：</p>
<ul>
<li>直接编码类别与目标变量之间的统计关系，为模型提供了强烈的预测信号</li>
<li>编码值具有明确的业务含义</li>
</ul>
<p>适用条件：需要足够样本量支撑</p>
<h2 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h2><h3 id="一、维数灾难-过多特征"><a href="#一、维数灾难-过多特征" class="headerlink" title="一、维数灾难 - 过多特征"></a>一、维数灾难 - 过多特征</h3><p>维数的定义：数据集中特征的数量</p>
<p>维数灾难的定义：随着数据维度增加，数据分析和建模变得复杂和低效的现象</p>
<p>随着维度的增加会带来的表现：</p>
<ul>
<li>样本空间的稀疏性：高纬度空间里面数据点分散</li>
<li>高维球体中，数据分布集中在表面</li>
<li>距离度量失效：如欧式距离无法区分样本</li>
<li>特征相关和冗余：维数增加，特征变得高度相关和冗余，增加计算复杂性</li>
<li>计算复杂度增加</li>
<li>过拟合：高维数据特征数量远多于样本数量时，模型容易过拟合</li>
</ul>
<h4 id="特征选择-1"><a href="#特征选择-1" class="headerlink" title="特征选择"></a>特征选择</h4><h5 id="定义：从原始特征集合中筛选出与目标变量高度相关、冗余度低的特征子集，保留对模型最有价值的信息"><a href="#定义：从原始特征集合中筛选出与目标变量高度相关、冗余度低的特征子集，保留对模型最有价值的信息" class="headerlink" title="定义：从原始特征集合中筛选出与目标变量高度相关、冗余度低的特征子集，保留对模型最有价值的信息"></a>定义：从原始特征集合中筛选出与目标变量高度相关、冗余度低的特征子集，保留对模型最有价值的信息</h5><p>特征选择的方法：</p>
<ul>
<li><strong>过滤法</strong>：只看特征的【统计特性】，和模型训练没关系（独立于模型）</li>
<li><strong>封装法</strong>：用模型性能作为特征选择的评估标准，通过反复训练模型来筛选特征</li>
<li><strong>嵌入法</strong>：特征选择和模型训练“绑在一起”，通过模型给出的“特征重要性”来选择特征</li>
</ul>
<h3 id="二、过滤法"><a href="#二、过滤法" class="headerlink" title="二、过滤法"></a>二、过滤法</h3><p>定义：基于特征的统计特性进行特征选择，独立于机器学习算法</p>
<p>适用场景：大规模数据初步筛选</p>
<h5 id="过滤法会筛选掉哪些数据："><a href="#过滤法会筛选掉哪些数据：" class="headerlink" title="过滤法会筛选掉哪些数据："></a>过滤法会筛选掉哪些数据：</h5><ul>
<li><u>冗余</u>或<u>与目标不相关</u>的特征</li>
<li><u>方差较低</u>的特征</li>
<li><u>取值几乎不变</u>的特征</li>
<li><u>强相关</u>的特征：计算所有特征间的相关系数，删除和其他特征平均相似度更【高】的特征，重复，直到任何两个特征之间的相关系数低于某个临界值</li>
<li>特征进行<u>聚类</u>：从每一类选少数几个有代表性的特征</li>
</ul>
<h4 id="1）相关系数法"><a href="#1）相关系数法" class="headerlink" title="1）相关系数法"></a>1）相关系数法</h4><h5 id="皮尔逊相关系数（SIS）-两个数值型变量的线性相关程度"><a href="#皮尔逊相关系数（SIS）-两个数值型变量的线性相关程度" class="headerlink" title="皮尔逊相关系数（SIS）- 两个数值型变量的线性相关程度"></a>皮尔逊相关系数（SIS）- 两个数值型变量的线性相关程度</h5><p>通过衡量每个特征与目标变量的皮尔逊相关系数来衡量它们之间的重要性</p>
<p>作用：衡量“特征与目标变量”的线性关系强度，从而筛选无关特征</p>
<p>适用场景：【数值型特征】的【线性关系】评估</p>
<p>注：皮尔逊相关系数只能捕捉【线性关系】，无法识别特征与目标间的非线性关系</p>
<h5 id="补充：皮尔逊相关系数的计算"><a href="#补充：皮尔逊相关系数的计算" class="headerlink" title="补充：皮尔逊相关系数的计算"></a>补充：皮尔逊相关系数的计算</h5><p>核心：衡量两个数值型变量（”特征X“和”目标Y“）的线性相关程度</p>
<p>样本数：n</p>
<p>特征X取值：<em>x</em>1,<em>x</em>2,…,<em>x</em>n</p>
<p>特征Y取值：<em>y</em>1,<em>y</em>2,…,<em>y</em>n</p>
<ul>
<li><p>计算X和Y的平均值</p>
<img src="/2025/12/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/18.png" class="" title="图片描述">
</li>
<li><p>计算“偏差乘积和”</p>
<img src="/2025/12/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/19.png" class="" title="图片描述">
</li>
<li><p>分别计算X和Y的”偏差平方和的平方根“</p>
<img src="/2025/12/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/20.png" class="" title="图片描述">
</li>
<li><p>皮尔逊相关系数计算</p>
<img src="/2025/12/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/21.png" class="" title="图片描述">

<ul>
<li>r 的取值范围：[-1,1]<ul>
<li>靠近 1：XY正线性相关</li>
<li>靠近 -1：XY负线性相关</li>
<li>靠近 0：XY线性无关</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="2）卡方检验-两个离散型变量之间的独立性"><a href="#2）卡方检验-两个离散型变量之间的独立性" class="headerlink" title="2）卡方检验 - 两个离散型变量之间的独立性"></a>2）卡方检验 - 两个离散型变量之间的独立性</h4><p>卡方检验用于判断”离散型特征和离散型目标变量“之间的独立性</p>
<p>通过案例来解释：</p>
<img src="/2025/12/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/22.png" class="" title="图片描述">

<p>核心步骤：</p>
<ul>
<li><p>提出【原假设】和【备择假设】</p>
<img src="/2025/12/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/23.png" class="" title="图片描述">
</li>
<li><p>构造列联表，统计观测频数：<em>O</em>i</p>
<img src="/2025/12/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/24.png" class="" title="图片描述">
</li>
<li><p>计算期望频数：<em>E</em>i</p>
<p>计算公式：</p>
<img src="/2025/12/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/26.png" class="" title="图片描述">

<p>案例：</p>
<img src="/2025/12/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/25.png" class="" title="图片描述">

<p>计算出结果：</p>
<img src="/2025/12/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/27.png" class="" title="图片描述">
</li>
<li><p>计算卡方统计量：<em>χ</em>2</p>
<p>对每个单元格，算 “（观测频数 - 期望频数）的平方 ÷ 期望频数”，再把所有结果加起来，得到卡方值</p>
<img src="/2025/12/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/28.png" class="" title="图片描述">
</li>
<li><p>假设检验</p>
<p>卡方值越大，越拒绝 “两者独立” 的原假设，认为特征与目标相关</p>
<ul>
<li><p>确定原假设（<em>H</em>0）：支付方式与评分是独立的（没关系）</p>
</li>
<li><p>卡方统计量：<em>χ</em>2&#x3D;152.36</p>
</li>
<li><p>确定【自由度】</p>
<ul>
<li>公式：自由度 &#x3D;（行数 - 1）*（列数 - 1）</li>
</ul>
</li>
<li><p>确定【显著性水平】</p>
<ul>
<li><p>定义：有多少概率错误地推翻原假设</p>
</li>
<li><p><em>α</em>&#x3D;0.05（意思是：有 5% 的概率错误地推翻原假设）</p>
</li>
</ul>
</li>
<li><p>查表</p>
<ul>
<li>找：“自由度 &#x3D; 12，显著性水平 &#x3D; 0.05” 对应的<strong>临界值</strong></li>
<li>查出来是 21.03</li>
</ul>
</li>
<li><p>得出结论：</p>
<ul>
<li>案例中计算的卡方统计量是 152.36，远大于临界值 21.03</li>
<li>当 “卡方统计量&gt; 临界值” 时，说明 “实际与期望的差距太大，原假设（没关系）站不住脚”，因此<strong>拒绝原假设，接受备择假设</strong>—— 认为 “支付方式与评分显著相关”</li>
</ul>
</li>
</ul>
</li>
<li><p>总结：</p>
<ul>
<li>算出来的卡方越大，特征与目标的关联性越强</li>
<li>相应的卡方对应的p值（假设检验的显著性值）越小，特征越重要</li>
</ul>
</li>
</ul>
<h4 id="3）单变量方差分析（One-Way-ANOVA）-检测离散特征（分类变量）对数值型目标变量（连续变量）的影响是否显著-检验数值型特征和离散型目标变量的显著性"><a href="#3）单变量方差分析（One-Way-ANOVA）-检测离散特征（分类变量）对数值型目标变量（连续变量）的影响是否显著-检验数值型特征和离散型目标变量的显著性" class="headerlink" title="3）单变量方差分析（One-Way ANOVA）- 检测离散特征（分类变量）对数值型目标变量（连续变量）的影响是否显著 &#x2F; 检验数值型特征和离散型目标变量的显著性"></a>3）单变量方差分析（One-Way ANOVA）- 检测离散特征（分类变量）对数值型目标变量（连续变量）的影响是否显著 &#x2F; 检验数值型特征和离散型目标变量的显著性</h4><p>通过案例了解：</p>
<img src="/2025/12/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/29.png" class="" title="图片描述">

<p>解答步骤：</p>
<ul>
<li><p>提出原假设</p>
<img src="/2025/12/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/30.png" class="" title="图片描述">
</li>
<li><p>计算总平均购买均值 （总均值）</p>
</li>
<li><p>按用户等级分组，计算每组的组内平均金额（组内均值）</p>
<img src="/2025/12/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/31.png" class="" title="图片描述">
</li>
<li><p>计算【组间平方和 SSB】和【组间均方 MSB】</p>
</li>
<li><p>计算【组内平方和 SSW】和【组内均方 MSW】</p>
</li>
<li><p>计算【统计量 F】&#x3D; 组间均方 &#x2F; 组内均方</p>
<img src="/2025/12/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/32.png" class="" title="图片描述">

<ul>
<li>F值越大，相关性越高，组间差异越显著，越推翻原假设，认为特征对目标有显著影响</li>
</ul>
</li>
<li><p>通过p值下结论</p>
<ul>
<li>p值为”原假设成立的概率“，p值太小（&lt; 0.05），推翻原假设，认为用户等级对购买金额有显著影响</li>
</ul>
</li>
</ul>
<p>例（SSB和SSW的计算）：</p>
<p>案例：</p>
<img src="/2025/12/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/33.png" class="" title="图片描述">

<p>SSB 组间平方和：</p>
<img src="/2025/12/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/34.png" class="" title="图片描述">

<p>SSW 组内平方和：</p>
<img src="/2025/12/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/35.png" class="" title="图片描述">

<ul>
<li><p>总结</p>
<img src="/2025/12/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/36.png" class="" title="图片描述"></li>
</ul>
<h3 id="三、封装法"><a href="#三、封装法" class="headerlink" title="三、封装法"></a>三、封装法</h3><p>过滤法的缺点：</p>
<ul>
<li>独立评估每个特征（看每个特征和目标的相关性），不考虑特征之间的相互作用</li>
</ul>
<h4 id="1）封装法定义"><a href="#1）封装法定义" class="headerlink" title="1）封装法定义"></a>1）封装法定义</h4><p>通过反复训练模型来评估特征的重要性，选择能提高模型性能的特征子集。</p>
<p>常见封装法：</p>
<ul>
<li>递归特征消除</li>
<li>前向选择</li>
<li>后向消除</li>
</ul>
<h4 id="2）递归特征消除"><a href="#2）递归特征消除" class="headerlink" title="2）递归特征消除"></a>2）递归特征消除</h4><p>定义：反复训练模型，每次把”当前最没用的“特征淘汰掉（通过计算特征重要性），直到剩下想要的特征数量</p>
<p>前提条件：模型必须能输出特征重要性（如：线性模型中的权重系数、决策树中的信息熵）</p>
<p>步骤：</p>
<ul>
<li>初始化：输入完整数据集（包含所有特征）；设定最终要选择的特征数量k</li>
<li>训练模型并获取重要性，对特征重要性排序</li>
<li>剔除重要性排名最低的特征</li>
<li>递归循环，最终输出结果</li>
</ul>
<p>优势：能捕捉特征间的依赖&#x2F;冗余关系（删除某特征后，其他特征的重要性会重新分配）</p>
<h4 id="3）前向选择"><a href="#3）前向选择" class="headerlink" title="3）前向选择"></a>3）前向选择</h4><p>前向选择和后向消除都属于【<u>逐步回归法</u>】的范畴，通过贪婪搜索策略来寻找最优特征子集</p>
<h5 id="什么是前向选择？"><a href="#什么是前向选择？" class="headerlink" title="什么是前向选择？"></a>什么是前向选择？</h5><p>从一个特征都没有，每次选一个加进去能使模型效果提升最多的特征，直到特征数量达到目标数量</p>
<p>步骤：</p>
<ul>
<li>初始化：空特征子集 S &#x3D; {空}；设定所有特征为候选集 C &#x3D; {<em>X</em>1，<em>X</em>2，…，<em>X</em>p}</li>
<li>评估候选特征：对每个候选特征创建临时特征集，在临时特征集上训练模型，用交叉验证评估性能，选择能带来最大性能提升的候选特征</li>
<li>选择最佳特征</li>
<li>重复，直到满足停止条件</li>
</ul>
<img src="/2025/12/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/37.png" class="" title="图片描述">

<h4 id="4）后向消除"><a href="#4）后向消除" class="headerlink" title="4）后向消除"></a>4）后向消除</h4><h5 id="什么是后向消除？"><a href="#什么是后向消除？" class="headerlink" title="什么是后向消除？"></a>什么是后向消除？</h5><p>从全部特征开始，每次删去一个去掉后模型效果下降最小的特征，直到剩下的特征数目达到目标数目</p>
<p>步骤：</p>
<ul>
<li>初始化：完整特征集合</li>
<li>评估特征重要性：对当前特征集中的每个特征创建临时特征集，在临时特征集上训练模型，交叉验证评估性能，移除对性能影响最小的特征</li>
<li>移除最不重要的特征</li>
<li>重复，直到满足停止条件</li>
</ul>
<img src="/2025/12/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/38.png" class="" title="图片描述">

<h4 id="5）双向搜索"><a href="#5）双向搜索" class="headerlink" title="5）双向搜索"></a>5）双向搜索</h4><h5 id="前向选择"><a href="#前向选择" class="headerlink" title="前向选择"></a>前向选择</h5><ul>
<li>优：速度快；倾向于选择较小的特征子集</li>
<li>缺：特征一旦加入不能删除，可能错过更好的组合</li>
</ul>
<h5 id="后向消除"><a href="#后向消除" class="headerlink" title="后向消除"></a>后向消除</h5><ul>
<li>优：更好看出特征之间的关联</li>
<li>缺：特征多的话，速度慢</li>
</ul>
<h5 id="双向搜索：每一步，即考虑添加新特征；又考虑移除已有特征"><a href="#双向搜索：每一步，即考虑添加新特征；又考虑移除已有特征" class="headerlink" title="双向搜索：每一步，即考虑添加新特征；又考虑移除已有特征"></a>双向搜索：每一步，即考虑添加新特征；又考虑移除已有特征</h5><ul>
<li>空集&#x2F;小集合开始</li>
<li>添加一个最能提升性能的特征（前向）</li>
<li>检查已选择的特征中是否有可以移除但不影响性能的（后向）</li>
<li>重复</li>
</ul>
<h3 id="四、嵌入法"><a href="#四、嵌入法" class="headerlink" title="四、嵌入法"></a>四、嵌入法</h3><h4 id="1）嵌入法定义"><a href="#1）嵌入法定义" class="headerlink" title="1）嵌入法定义"></a>1）嵌入法定义</h4><p>定义：嵌入法将“特征选择”过程与“模型训练”过程相结合，<strong>在模型训练时通过特征的重要性自动选择特征</strong>。</p>
<h5 id="特征选择：选择对模型重要的特征"><a href="#特征选择：选择对模型重要的特征" class="headerlink" title="特征选择：选择对模型重要的特征"></a>特征选择：选择对模型重要的特征</h5><p>优点：更加【高效】</p>
<h5 id="嵌入法-v-s-封装法："><a href="#嵌入法-v-s-封装法：" class="headerlink" title="嵌入法 v.s. 封装法："></a>嵌入法 v.s. 封装法：</h5><ul>
<li>嵌入法：一次训练完成 “选特征 + 训练模型”</li>
<li>封装法：多次训练模型，尝试不同特征组合</li>
</ul>
<h5 id="嵌入法的两种实现方式："><a href="#嵌入法的两种实现方式：" class="headerlink" title="嵌入法的两种实现方式："></a>嵌入法的两种实现方式：</h5><ul>
<li>正则化</li>
<li>基于树模型的特征选择</li>
</ul>
<h4 id="2）正则化"><a href="#2）正则化" class="headerlink" title="2）正则化"></a>2）正则化</h4><h5 id="定义：正则化会在模型的【损失函数】中添加一个额外的正则项，这个正则项会“惩罚”模型使用过多或过大的系数"><a href="#定义：正则化会在模型的【损失函数】中添加一个额外的正则项，这个正则项会“惩罚”模型使用过多或过大的系数" class="headerlink" title="定义：正则化会在模型的【损失函数】中添加一个额外的正则项，这个正则项会“惩罚”模型使用过多或过大的系数"></a>定义：正则化会在模型的【损失函数】中添加一个额外的正则项，这个正则项会“惩罚”模型使用过多或过大的系数</h5><p>人话：在模型预测不准确的损失中额外加一项“模型系数太复杂的惩罚”，促使模型使用更加简单的方式预测</p>
<p>模型为了最小化这个惩罚的总损失，会将不重要的特征系数压缩至“0”，只保留对预测模型影响足够大的特征。</p>
<p>因此，正则化使在模型完成训练时同时也自动完成了特征的选择（系数 &#x3D; 0的特征被删除；系数不为0的特征被保留）</p>
<h5 id="以线性回归为例："><a href="#以线性回归为例：" class="headerlink" title="以线性回归为例："></a>以线性回归为例：</h5><p>【不带正则项】的线性回归模型，式子目标为：预测值和真实值的误差平方和最小</p>
<ul>
<li><p>其中，符号含义</p>
<img src="/2025/12/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/41.png" class="" title="图片描述"></li>
</ul>
<p>公式：</p>
<img src="/2025/12/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/39.png" class="" title="图片描述">

<p>这个式子的缺点：不关心系数w的大小或稀疏性，容易导致【过拟合】</p>
<p>【加入正则化后的损失函数】：</p>
<ul>
<li><p>其中 <em>L</em>(w) 为原始的损失函数，<em>P</em>(w) 为正则项，<em>λ</em> 为正则化强化参数（控制对模型复杂度的惩罚力度）</p>
<img src="/2025/12/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/42.png" class="" title="图片描述"></li>
</ul>
<p>公式：</p>
<img src="/2025/12/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/40.png" class="" title="图片描述">

<ul>
<li><em>λ</em> 越大，惩罚力度越强 ，模型倾向于使用更小、更少的系数，模型更简单</li>
<li><em>λ</em> 越小，惩罚力度越弱，模型倾向于拟合训练数据，模型更复杂</li>
</ul>
<img src="/2025/12/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/43.png" class="" title="图片描述">

<ul>
<li>从左到右，惩罚力度越来越强</li>
<li>正则化的惩罚力度（λ）越大，特征的系数越容易被压缩到 0，模型越简单</li>
</ul>
<h4 id="3）LASSO回归（L1正则化）"><a href="#3）LASSO回归（L1正则化）" class="headerlink" title="3）LASSO回归（L1正则化）"></a>3）LASSO回归（L1正则化）</h4><p>将正则化的惩罚项换为：<strong>所有特征系数的绝对值之和</strong></p>
<img src="/2025/12/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/44.png" class="" title="图片描述">

<h5 id="L1正则化的几何解释与特征选择机制："><a href="#L1正则化的几何解释与特征选择机制：" class="headerlink" title="L1正则化的几何解释与特征选择机制："></a>L1正则化的几何解释与特征选择机制：</h5><img src="/2025/12/09/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/45.png" class="" title="图片描述">

<ul>
<li>图中 β1 和 β2 为模型里两个特征对应的系数</li>
<li>L1正则化在几何上是一个菱形，有尖角，损失函数的等高线在与这个菱形相交时，大概率在尖角处相切，而这一尖角的位置对应着某些特征系数为0的情况</li>
<li>特征选择效果：Lasso回归会训练出一个稀疏的系数向量 w，让很多特征的系数精确 &#x3D; 0，从而实现特征选择</li>
</ul>

    </div>
    
    
    
    
    
    
    
</div>

            <footer id="footer">
    <div id="footer-wrap">
        <div>
            &copy;
            2005 - 2026 yty的博客
            <span id="footer-icon">
                <i class="fa-solid fa-font-awesome fa-fw"></i>
            </span>
            &commat;Leo yu yty
        </div>
        <div>
            Based on the <a target="_blank" rel="noopener" href="https://hexo.io">Hexo Engine</a> &amp;
            <a target="_blank" rel="noopener" href="https://github.com/theme-particlex/hexo-theme-particlex">ParticleX Theme</a>
        </div>
        
    </div>
</footer>

        </div>
        
        <transition name="fade">
            <div id="preview" ref="preview" v-show="previewShow">
                <img id="preview-content" ref="previewContent" />
            </div>
        </transition>
        
    </div>
    <script src="/js/main.js"></script>
    
    




    
</body>
</html>
