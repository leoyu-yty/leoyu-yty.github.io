
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="utf-8" />
    <title>机器学习-神经网络 | yty的博客</title>
    <meta name="author" content="Leo yu yty" />
    <meta name="description" content="Hello Everyone This is my website" />
    <meta name="keywords" content="" />
    <meta
        name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0"
    />
    <link rel="icon" href="/images/xnylh.webp" />
    <link rel="preconnect" href="https://s4.zstatic.net" />
<script src="https://s4.zstatic.net/ajax/libs/vue/3.3.7/vue.global.prod.min.js"></script>
<link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/font-awesome/6.4.2/css/all.min.css" />
<link rel="preconnect" href="https://fonts.googleapis.cn" />
<link rel="preconnect" href="https://fonts.gstatic.cn" crossorigin />
<link
    rel="stylesheet"
    href="https://fonts.googleapis.cn/css2?family=Fira+Code:wght@400;500;600;700&family=Lexend:wght@400;500;600;700;800;900&family=Noto+Sans+SC:wght@400;500;600;700;800;900&display=swap"
/>
<script> const mixins = {}; </script>

<script src="https://polyfill.alicdn.com/v3/polyfill.min.js?features=default"></script>


<script src="https://s4.zstatic.net/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script src="https://s4.zstatic.net/ajax/libs/highlightjs-line-numbers.js/2.8.0/highlightjs-line-numbers.min.js"></script>
<link
    rel="stylesheet"
    href="https://s4.zstatic.net/ajax/libs/highlight.js/11.9.0/styles/github.min.css"
/>
<script src="/js/lib/highlight.js"></script>



<script src="/js/lib/preview.js"></script>









<link rel="stylesheet" href="/css/main.css" />

<meta name="generator" content="Hexo 8.1.1"></head>
<body>
    <div id="layout">
        <transition name="fade">
            <div id="loading" v-show="loading">
                <div id="loading-circle">
                    <h2>LOADING</h2>
                    <p>加载过慢请开启缓存 浏览器默认开启</p>
                    <img src="/images/loading.gif" />
                </div>
            </div>
        </transition>
        <div id="menu" :class="{ hidden: hiddenMenu, 'menu-color': menuColor}">
    <nav id="desktop-menu">
        <a class="title" href="/">
            <span>YTY的博客</span>
        </a>
        
        <a href="/">
            <i class="fa-solid fa-house fa-fw"></i>
            <span>&ensp;主页</span>
        </a>
        
        <a href="/about/">
            <i class="fa-solid fa-id-card fa-fw"></i>
            <span>&ensp;关于</span>
        </a>
        
        <a href="/archives/">
            <i class="fa-solid fa-box-archive fa-fw"></i>
            <span>&ensp;文章</span>
        </a>
        
        <a href="/categories/">
            <i class="fa-solid fa-bookmark fa-fw"></i>
            <span>&ensp;分类</span>
        </a>
        
        <a href="/tags/">
            <i class="fa-solid fa-tags fa-fw"></i>
            <span>&ensp;标签</span>
        </a>
        
    </nav>
    <nav id="mobile-menu">
        <div class="title" @click="showMenuItems = !showMenuItems">
            <i class="fa-solid fa-bars fa-fw"></i>
            <span>&emsp;YTY的博客</span>
        </div>
        <transition name="slide">
            <div class="items" v-show="showMenuItems">
                
                <a href="/">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-house fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">主页</div>
                    </div>
                </a>
                
                <a href="/about/">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-id-card fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">关于</div>
                    </div>
                </a>
                
                <a href="/archives/">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-box-archive fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">文章</div>
                    </div>
                </a>
                
                <a href="/categories/">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-bookmark fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">分类</div>
                    </div>
                </a>
                
                <a href="/tags/">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-tags fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">标签</div>
                    </div>
                </a>
                
            </div>
        </transition>
    </nav>
</div>
<transition name="fade">
    <div id="menu-curtain" @click="showMenuItems = !showMenuItems" v-show="showMenuItems"></div>
</transition>

        <div id="main" :class="loading ? 'into-enter-from': 'into-enter-active'">
            <div class="article">
    <div>
        <h1>机器学习-神经网络</h1>
    </div>
    <div class="info">
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2025/12/25
        </span>
        
        <span class="category">
            <a href="/categories/%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                复习笔记
            </a>
        </span>
        
        
        <span class="tags">
            <span class="icon">
                <i class="fa-solid fa-tags fa-fw"></i>
            </span>
            
            
            <span class="tag">
                
                <a href="/tags/%E7%AC%94%E8%AE%B0/" style="color: #ff7d73">
                    笔记
                </a>
            </span>
            
            <span class="tag">
                
                <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/" style="color: #00bcd4">
                    机器学习基础
                </a>
            </span>
            
        </span>
        
    </div>
    
    <div class="content" v-pre>
        <h2 id="摘要部分"><a href="#摘要部分" class="headerlink" title="摘要部分"></a>摘要部分</h2><h4 id="一、神经网络概念"><a href="#一、神经网络概念" class="headerlink" title="一、神经网络概念"></a>一、神经网络概念</h4><p>神经网络中的最基本单元：神经元</p>
<p>模拟【生物神经元的激励信号和抑制信号】</p>
<h4 id="二、感知机"><a href="#二、感知机" class="headerlink" title="二、感知机"></a>二、感知机</h4><p>（（输入信号 * 权重）求和 + 偏置项）这一结果通过模型的【激活函数】得到最终输出</p>
<img src="/2025/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2.png" class="" title="图片描述">

<h4 id="三、多层感知机"><a href="#三、多层感知机" class="headerlink" title="三、多层感知机"></a>三、多层感知机</h4><img src="/2025/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/5.png" class="" title="图片描述">

<p>1、前馈网络</p>
<p>2、激活函数</p>
<p>3、普适逼近定理 —— 【非线性】变换对提升模型的表达能力十分重要</p>
<p>4、反向传播 —— 链式法则 &amp; 梯度下降法更新参数 &amp; 损失函数</p>
<span id="more"></span>







<h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><h3 id="一、神经网络相关定义"><a href="#一、神经网络相关定义" class="headerlink" title="一、神经网络相关定义"></a>一、神经网络相关定义</h3><ul>
<li>人工神经网络（ANN）</li>
<li>神经网络中的最基本单元：神经元</li>
</ul>
<h3 id="二、神经网络雏形"><a href="#二、神经网络雏形" class="headerlink" title="二、神经网络雏形"></a>二、神经网络雏形</h3><ul>
<li><p>神经元的信号处理公式：</p>
<ul>
<li><p>假设有很多个神经元（编号 i&#x3D;1 到 N_j）给神经元 j 发信号（信号是 O_i），神经元 j 会给每个收到的信号分配一个 “权重” w_ji，把 “信号 × 权重” 全部加起来，得到一个总和 z_j</p>
<img src="/2025/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/1.png" class="" title="图片描述">
</li>
<li><p>当处理结果 z_j &lt; T_j （T_j为一个阈值）时，神经元 j 的输出 o_j &#x3D; 0，反之输出 o_j &#x3D; 1</p>
</li>
<li><p>这一设计：模拟【生物神经元的激励信号和抑制信号】</p>
</li>
<li><h5 id="弊端：1、表达能力有限；2、每个神经元的参数需要人为指定"><a href="#弊端：1、表达能力有限；2、每个神经元的参数需要人为指定" class="headerlink" title="弊端：1、表达能力有限；2、每个神经元的参数需要人为指定"></a>弊端：1、表达能力有限；2、每个神经元的参数需要人为指定</h5></li>
</ul>
</li>
</ul>
<h3 id="三、感知机-引入【偏置项（b）】-激活函数"><a href="#三、感知机-引入【偏置项（b）】-激活函数" class="headerlink" title="三、感知机 - 引入【偏置项（b）】+ 激活函数"></a>三、感知机 - 引入【偏置项（b）】+ 激活函数</h3><ul>
<li><p>神经网络的弊端：权重需要人为指定</p>
</li>
<li><p>组成：（（输入信号*权重）求和 + 偏置项）这一结果通过模型的【激活函数】得到最终输出</p>
</li>
<li><p>感知机的结构：</p>
<ul>
<li><p>结构构成：输入x、权重w、偏置b、激活函数ϕ、输出y</p>
</li>
<li><h5 id="偏置项b："><a href="#偏置项b：" class="headerlink" title="偏置项b："></a>偏置项b：</h5><ul>
<li><h5 id="提高线性模型的表达能力"><a href="#提高线性模型的表达能力" class="headerlink" title="提高线性模型的表达能力"></a>提高线性模型的表达能力</h5>如：原来的线性函数（比如 y&#x3D;kx）一定过原点，加了 b 之后变成 y&#x3D;kx+b，从而能表示平面上任意一条直线</li>
</ul>
</li>
<li><h5 id="激活函数ϕ："><a href="#激活函数ϕ：" class="headerlink" title="激活函数ϕ："></a>激活函数ϕ：</h5><ul>
<li><h5 id="模拟神经元的兴奋和抑制状态"><a href="#模拟神经元的兴奋和抑制状态" class="headerlink" title="模拟神经元的兴奋和抑制状态"></a>模拟神经元的兴奋和抑制状态</h5><p>在感知机中，激活函数通常是示性函数 I(z ≥ 0)。</p>
<ul>
<li>当输入 z 非负时，函数输出 1，对应兴奋状态</li>
<li>当输入 z 为负数时，函数输出 0，对应抑制状态</li>
</ul>
</li>
</ul>
</li>
</ul>
<img src="/2025/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2.png" class="" title="图片描述">
</li>
<li><h5 id="感知机用于解决【二分类问题】"><a href="#感知机用于解决【二分类问题】" class="headerlink" title="感知机用于解决【二分类问题】"></a>感知机用于解决【二分类问题】</h5></li>
<li><img src="/2025/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/3.png" class="" title="图片描述"></li>
</ul>
<h3 id="四、多层感知机"><a href="#四、多层感知机" class="headerlink" title="四、多层感知机"></a>四、多层感知机</h3><ul>
<li>单层感知机只能解决【线性问题】，解决不了【异或问题】</li>
<li>多层感知机：将一个感知机的输出作为输入，连接到下一个感知机上<ul>
<li>如果一个感知机对应平面上的一条直线，那么多个感知机就可以将平面分割成多边形区域，达到【超越线性】的效果</li>
<li>多层感知机的典型应用：解决【异或问题】</li>
</ul>
</li>
</ul>
<h4 id="1）前馈"><a href="#1）前馈" class="headerlink" title="1）前馈"></a>1）前馈</h4><ul>
<li><p>定义：将神经元分为不同的层，每一层只与其前后相邻层的神经元连接，层内以及间隔一层以上的神经元之间没有连接</p>
</li>
<li><p>前馈神经网络</p>
<ul>
<li><img src="/2025/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/5.png" class="" title="图片描述"></li>
</ul>
</li>
<li><h5 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h5><ul>
<li>输入层 —— 直接接收输入</li>
<li>隐含层 —— 中间处理部分</li>
<li>输出层 —— 最终输出结果</li>
</ul>
</li>
<li><p>【前馈网络的层数 &#x3D; 权重的层数】 &#x3D; 边的层数，神经元上不携带权重</p>
</li>
<li><h5 id="多层感知机定义"><a href="#多层感知机定义" class="headerlink" title="多层感知机定义"></a>多层感知机定义</h5><ul>
<li>将多个单层感知机按前馈结构组合，就形成了多层感知机</li>
</ul>
</li>
<li><h5 id="感知机与多层感知机的对比"><a href="#感知机与多层感知机的对比" class="headerlink" title="感知机与多层感知机的对比"></a>感知机与多层感知机的对比</h5><ul>
<li><img src="/2025/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/4.png" class="" title="图片描述"></li>
</ul>
</li>
<li><p>例：3层感知机的输入输出过程</p>
<ul>
<li><img src="/2025/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/6.png" class="" title="图片描述"></li>
<li><img src="/2025/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/7.png" class="" title="图片描述"></li>
</ul>
</li>
</ul>
<h4 id="2）激活函数-——【必须是非线性】"><a href="#2）激活函数-——【必须是非线性】" class="headerlink" title="2）激活函数 ——【必须是非线性】"></a>2）激活函数 ——【必须是非线性】</h4><ul>
<li>常见激活函数<ul>
<li>逻辑斯谛函数（Sigmoid）—— 处理二分类问题</li>
<li>双曲正切函数（Tanh）</li>
<li>线性整流单元（ReLU）—— 最常作为隐含层的激活函数</li>
<li>Softmax —— 处理多分类问题</li>
</ul>
</li>
</ul>
<h3 id="3）普适逼近定理-神经网络的理论基础"><a href="#3）普适逼近定理-神经网络的理论基础" class="headerlink" title="3）普适逼近定理 - 神经网络的理论基础"></a>3）普适逼近定理 - 神经网络的理论基础</h3><ul>
<li><p>定理内容：</p>
<p>任意一个定义在 Rn 上的连续函数，都可以由大小合适的多层感知机模型来拟合</p>
<p>（即：不管多复杂的连续函数，多层感知机模型都能学习）</p>
<p>只要满足两个条件：</p>
<ul>
<li>具有<u>至少一层隐藏层</u>的前馈神经网络（也叫：多层感知机），且<u>隐藏层神经元数量有限</u></li>
<li><u>激活函数满足特定性质</u>（如：Sigmoid、Tanh、ReLU）</li>
</ul>
</li>
<li><h5 id="官方定义："><a href="#官方定义：" class="headerlink" title="官方定义："></a>官方定义：</h5><p>一个具有至少一层隐藏层的前馈神经网络（多层感知机），若藏层包含有限数量神经元，且激活函数满足特定性质（如：Sigmoid、Tanh、ReLU），则可以任意精度逼近定义在 Rn 闭集上的任意连续函数</p>
</li>
<li><h5 id="普适逼近定理的意义："><a href="#普适逼近定理的意义：" class="headerlink" title="普适逼近定理的意义："></a>普适逼近定理的意义：</h5><ul>
<li>理论上证明了多层神经网络的表达能力</li>
<li>为深度学习提供理论基础</li>
</ul>
</li>
<li><h5 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h5><ul>
<li>【非线性】变换对提升模型的表达能力十分重要</li>
</ul>
</li>
<li><h5 id="异或问题的解决"><a href="#异或问题的解决" class="headerlink" title="异或问题的解决"></a>异或问题的解决</h5><ul>
<li>数据维度提升的好处在于：在低维空间中线性不可分的数据，经过合适的非线性变换，在高维空间中可能变得线性可分。</li>
<li>多层感知机解决异或问题的核心逻辑：通过非线性激活函数提升数据维度，实现线性可分，从而提升表达能力。</li>
</ul>
</li>
</ul>
<h3 id="四、反向传播"><a href="#四、反向传播" class="headerlink" title="四、反向传播"></a>四、反向传播</h3><ul>
<li><h5 id="反向传播作用："><a href="#反向传播作用：" class="headerlink" title="反向传播作用："></a>反向传播作用：</h5><ul>
<li>调节多层感知机的参数（权重w，偏置b）</li>
</ul>
<p>人话：</p>
<ul>
<li>通过计算 “预测值和真实值的差距”，反过来调整前馈网络中的权重项w和偏置项b，让下次预测更准</li>
</ul>
</li>
<li><h5 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h5><ul>
<li>衡量 “预测值和真实值的差距” &#x3D; 损失值（<strong>J</strong>）<ul>
<li>差距越大，损失越大</li>
<li>差距越小，损失越小</li>
</ul>
</li>
</ul>
</li>
<li><h5 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h5><ul>
<li><p>每个参数对损失的影响**（∂J&#x2F;∂W、∂J&#x2F;∂b）**</p>
</li>
<li><p>本质：损失函数对参数的导数</p>
</li>
<li><h5 id="方向："><a href="#方向：" class="headerlink" title="方向："></a>方向：</h5><ul>
<li>梯度&gt;0，参数变大，损失变大</li>
<li>梯度&lt;0，参数变大，损失变小</li>
</ul>
</li>
<li><h5 id="大小："><a href="#大小：" class="headerlink" title="大小："></a>大小：</h5><ul>
<li>梯度绝对值越大，参数对损失影响越大</li>
<li>梯度绝对值越小，参数对损失影响越小</li>
</ul>
</li>
<li><img src="/2025/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/9.png" class="" title="图片描述"></li>
</ul>
</li>
<li><p>流向：</p>
<ul>
<li>”网络计算输出时的数据流向“和“损失函数求导时的梯度流向”相反</li>
<li>反向传播算法成立的本质：【链式求导法则】</li>
<li>靠后的层计算得到的结果可以在靠前的层中反复使用，无须在每一层都从头计算，大大【提高了梯度计算的效率】</li>
</ul>
</li>
<li><h5 id="反向传播算法的核心逻辑："><a href="#反向传播算法的核心逻辑：" class="headerlink" title="反向传播算法的核心逻辑："></a>反向传播算法的核心逻辑：</h5><ul>
<li><p>前馈计算：从输入层到输出层计算预测值，对比真实值，算出“损失”（也叫“误差”）</p>
</li>
<li><p>反向传播：从输出层往输入层【倒着算】，用【链式求导法则】，计算损失函数对每个参数的【梯度】（参数应该往哪个方向调，调多少），更新权重</p>
</li>
<li><p>参数更新：【梯度下降法】，按学习率 η 更新参数（W←W - η× 梯度，b←b - η× 梯度）</p>
<p>核心目标：【最小化损失函数 E(W)】（如平方损失、交叉熵损失）</p>
<img src="/2025/12/25/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/8.png" class="" title="图片描述"></li>
</ul>
</li>
<li><p>激活函数的选择</p>
<ul>
<li>深度学习：ReLU</li>
<li>输出层分类：Sigmoid（二分类），Softmax（多分类）</li>
<li>循环网络：Tanh，ReLU</li>
</ul>
</li>
<li><p>损失函数的选择</p>
<ul>
<li>平方损失：回归任务</li>
<li>交叉熵损失：分类任务</li>
</ul>
</li>
<li><h5 id="损失函数的作用"><a href="#损失函数的作用" class="headerlink" title="损失函数的作用"></a>损失函数的作用</h5><ul>
<li>衡量预测值和真实值的差异</li>
<li>为反向传播提供梯度方向（最小化损失）</li>
</ul>
</li>
</ul>

    </div>
    
    
    
    
    
    
    
</div>

            <footer id="footer">
    <div id="footer-wrap">
        <div>
            &copy;
            2005 - 2026 yty的博客
            <span id="footer-icon">
                <i class="fa-solid fa-font-awesome fa-fw"></i>
            </span>
            &commat;Leo yu yty
        </div>
        <div>
            Based on the <a target="_blank" rel="noopener" href="https://hexo.io">Hexo Engine</a> &amp;
            <a target="_blank" rel="noopener" href="https://github.com/theme-particlex/hexo-theme-particlex">ParticleX Theme</a>
        </div>
        
    </div>
</footer>

        </div>
        
        <transition name="fade">
            <div id="preview" ref="preview" v-show="previewShow">
                <img id="preview-content" ref="previewContent" />
            </div>
        </transition>
        
    </div>
    <script src="/js/main.js"></script>
    
    




    
</body>
</html>
